{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNXDRZyjTr6oYT5y8RtmcaN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/01PrathamS/MLOps/blob/main/logging_with_tensorboard.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "k0fwnrZ8iFOr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, ), (0.5, )),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(root=\"./data\", download=True, train=True, transform=transform)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", download=True, train=False, transform=transform)\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "\n",
        "train_dataset, val_dataset = random_split(train_dataset, [50000, 10000])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVmyjMBbjsBl",
        "outputId": "5574f107-23f2-4d7f-c0f7-4fd6c8ed11b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:01<00:00, 5071941.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 129682.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:01<00:00, 1245479.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3981301.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "iSxTCMSol1Fd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MNISTModel(nn.Module):\n",
        "\n",
        "  def __init__(self, input_shape: int,\n",
        "               hidden_units: int,\n",
        "               output_shape: int):\n",
        "    super().__init__()\n",
        "    self.block_1 = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=input_shape,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(in_channels=hidden_units,\n",
        "                  out_channels=hidden_units,\n",
        "                  kernel_size=3,\n",
        "                  stride=1,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=2,\n",
        "                     stride=2)\n",
        "    )\n",
        "\n",
        "    self.block_2 = nn.Sequential(\n",
        "        nn.Conv2d(hidden_units,\n",
        "                  hidden_units,\n",
        "                  3,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(hidden_units,\n",
        "                  hidden_units,\n",
        "                  3,\n",
        "                  padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(2)\n",
        "    )\n",
        "\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Flatten(),\n",
        "        nn.Linear(in_features=hidden_units*7*7,\n",
        "                  out_features=output_shape),\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.Tensor):\n",
        "    x = self.block_1(x)\n",
        "    x = self.block_2(x)\n",
        "    x = self.classifier(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "o9fDJtVumKNb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "torch.manual_seed(42)\n",
        "model = MNISTModel(input_shape=1,\n",
        "                   hidden_units=10,\n",
        "                   output_shape=len(class_names)).to(device)\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qg4g9arUnX7s",
        "outputId": "aaf8f6f7-fb43-4f14-eb28-4e5c2d6af435"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MNISTModel(\n",
              "  (block_1): Sequential(\n",
              "    (0): Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (block_2): Sequential(\n",
              "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Flatten(start_dim=1, end_dim=-1)\n",
              "    (1): Linear(in_features=490, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params=model.parameters(),\n",
        "                            lr=0.01)"
      ],
      "metadata": {
        "id": "fl0Dx-WJnq7C"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_fn(y_true, y_pred):\n",
        "  correct = torch.eq(y_true, y_pred).sum().item()\n",
        "  acc = (correct / len(y_pred)) * 100\n",
        "  return acc"
      ],
      "metadata": {
        "id": "Nd-4_lKmnzyq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(model: torch.nn.Module,\n",
        "               data_loader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               accuracy_fn,\n",
        "               device: torch.device = device):\n",
        "  train_loss, train_acc = 0, 0\n",
        "  model.to(device)\n",
        "  for batch, (X, y) in enumerate(data_loader):\n",
        "    X, y = X.to(device), y.to(device)\n",
        "    y_pred = model(X)\n",
        "\n",
        "    loss = loss_fn(y_pred, y)\n",
        "    train_loss += loss\n",
        "    train_acc += accuracy_fn(y_true=y,\n",
        "                             y_pred=y_pred.argmax(dim=1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "  train_loss /= len(data_loader)\n",
        "  train_acc /= len(data_loader)\n",
        "  print(f\"Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\")\n",
        "\n",
        "  return train_loss, train_acc"
      ],
      "metadata": {
        "id": "xxHV628cn79y"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_step(data_loader: torch.utils.data.DataLoader,\n",
        "              model: torch.nn.Module,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              accuracy_fn,\n",
        "              device: torch.device = device):\n",
        "\n",
        "  test_loss, test_acc = 0, 0\n",
        "  model.to(device)\n",
        "  model.eval()\n",
        "  with torch.inference_mode():\n",
        "    for X, y in data_loader:\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      test_pred = model(X)\n",
        "\n",
        "      test_loss += loss_fn(test_pred, y)\n",
        "      test_acc += accuracy_fn(y_true=y,\n",
        "                              y_pred=test_pred.argmax(dim=1))\n",
        "\n",
        "    test_loss /= len(data_loader)\n",
        "    test_acc /= len(data_loader)\n",
        "\n",
        "    print(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}%\\n\")\n",
        "\n",
        "  return test_loss, test_acc"
      ],
      "metadata": {
        "id": "QY4J5qiiorCm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter()"
      ],
      "metadata": {
        "id": "v1SytrHnpPCK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "results = {\n",
        "    \"train_loss\": [],\n",
        "    \"train_acc\" : [],\n",
        "    \"test_loss\": [],\n",
        "    \"test_acc\": []\n",
        "}\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "for epoch in tqdm(range(epochs)):\n",
        "  print(f\"Epoch: {epochs}\\n------------\")\n",
        "  train_loss, train_acc = train_step(data_loader=train_dataloader,\n",
        "                                     model=model,\n",
        "                                     loss_fn=loss_fn,\n",
        "                                     optimizer=optimizer,\n",
        "                                     accuracy_fn=accuracy_fn,\n",
        "                                     device=device\n",
        "                                     )\n",
        "\n",
        "  test_loss, test_acc = test_step(data_loader=test_dataloader,\n",
        "                                  model=model,\n",
        "                                  loss_fn=loss_fn,\n",
        "                                  accuracy_fn=accuracy_fn,\n",
        "                                  device=device)\n",
        "\n",
        "  print(\n",
        "        f\"Epoch: {epoch+1} | \"\n",
        "        f\"train_loss: {train_loss:.4f} | \"\n",
        "        f\"train_acc: {train_acc:.4f} | \"\n",
        "        f\"test_loss: {test_loss:.4f} | \"\n",
        "        f\"test_acc: {test_acc:.4f}\"\n",
        "      )\n",
        "\n",
        "  results[\"train_loss\"].append(train_loss)\n",
        "  results[\"train_acc\"].append(train_loss)\n",
        "  results[\"test_loss\"].append(test_loss)\n",
        "  results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "  writer.add_scalars(main_tag=\"Loss\",\n",
        "                     tag_scalar_dict={\"train_loss\": train_loss,\n",
        "                                      \"test_loss\": test_acc},\n",
        "                     global_step=epoch)\n",
        "\n",
        "  writer.add_scalars(main_tag=\"Accuracy\",\n",
        "                     tag_scalar_dict={\"train_acc\": train_acc,\n",
        "                                      \"test_acc\": test_acc},\n",
        "                     global_step=epoch)\n",
        "\n",
        "  writer.add_graph(model=model,\n",
        "                   input_to_model=torch.randn(10, 1, 28, 28).to(device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVbX6g_BpVax",
        "outputId": "8d0b0055-1daa-4752-8a0c-88eb6c70242b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/10 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.12126 | Train accuracy: 96.22%\n",
            "Test loss: 0.17483 | Test accuracy: 94.25%\n",
            "\n",
            "Epoch: 1 | train_loss: 0.1213 | train_acc: 96.2212 | test_loss: 0.1748 | test_acc: 94.2492\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 10%|█         | 1/10 [00:17<02:39, 17.73s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.09694 | Train accuracy: 96.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [00:35<02:22, 17.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.10394 | Test accuracy: 96.54%\n",
            "\n",
            "Epoch: 2 | train_loss: 0.0969 | train_acc: 96.9790 | test_loss: 0.1039 | test_acc: 96.5355\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.08198 | Train accuracy: 97.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 30%|███       | 3/10 [00:53<02:04, 17.78s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.06882 | Test accuracy: 97.94%\n",
            "\n",
            "Epoch: 3 | train_loss: 0.0820 | train_acc: 97.4548 | test_loss: 0.0688 | test_acc: 97.9433\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.07294 | Train accuracy: 97.71%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|████      | 4/10 [01:10<01:45, 17.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.06452 | Test accuracy: 97.93%\n",
            "\n",
            "Epoch: 4 | train_loss: 0.0729 | train_acc: 97.7127 | test_loss: 0.0645 | test_acc: 97.9333\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.06639 | Train accuracy: 97.98%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 5/10 [01:28<01:28, 17.77s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.05561 | Test accuracy: 98.22%\n",
            "\n",
            "Epoch: 5 | train_loss: 0.0664 | train_acc: 97.9846 | test_loss: 0.0556 | test_acc: 98.2228\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.06069 | Train accuracy: 98.13%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 60%|██████    | 6/10 [01:45<01:10, 17.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.05155 | Test accuracy: 98.28%\n",
            "\n",
            "Epoch: 6 | train_loss: 0.0607 | train_acc: 98.1266 | test_loss: 0.0516 | test_acc: 98.2827\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.05636 | Train accuracy: 98.29%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 70%|███████   | 7/10 [02:03<00:52, 17.48s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.04809 | Test accuracy: 98.45%\n",
            "\n",
            "Epoch: 7 | train_loss: 0.0564 | train_acc: 98.2945 | test_loss: 0.0481 | test_acc: 98.4525\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.05263 | Train accuracy: 98.35%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 80%|████████  | 8/10 [02:21<00:35, 17.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.04690 | Test accuracy: 98.34%\n",
            "\n",
            "Epoch: 8 | train_loss: 0.0526 | train_acc: 98.3545 | test_loss: 0.0469 | test_acc: 98.3427\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.05008 | Train accuracy: 98.49%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 9/10 [02:38<00:17, 17.55s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.04839 | Test accuracy: 98.48%\n",
            "\n",
            "Epoch: 9 | train_loss: 0.0501 | train_acc: 98.4905 | test_loss: 0.0484 | test_acc: 98.4824\n",
            "Epoch: 10\n",
            "------------\n",
            "Train loss: 0.04702 | Train accuracy: 98.59%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [02:56<00:00, 17.66s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.04286 | Test accuracy: 98.54%\n",
            "\n",
            "Epoch: 10 | train_loss: 0.0470 | train_acc: 98.5865 | test_loss: 0.0429 | test_acc: 98.5423\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %load_ext tensorboard\n",
        "# %tensorboard --logdir runs"
      ],
      "metadata": {
        "id": "cTP6tHIMqg5W"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_writer(experiment_name: str,\n",
        "                  model_name: str,\n",
        "                  extra: str=None) -> torch.utils.tensorboard.SummaryWriter():\n",
        "\n",
        "    from datetime import datetime\n",
        "    import os\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "    if extra:\n",
        "      log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name, extra)\n",
        "    else:\n",
        "      log_dir = os.path.join(\"runs\", timestamp, experiment_name, model_name)\n",
        "\n",
        "    print(f\"[INFO] Created SummaryWriter, saving to: {log_dir}...\")\n",
        "\n",
        "    return SummaryWriter(log_dir=log_dir)\n"
      ],
      "metadata": {
        "id": "hKwEDZhQsyQW",
        "outputId": "851e8c8c-8514-4ab3-d9cd-f929258f9194",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Created SummaryWriter, saving to: runs/2024-05-14/data_10_percent/effnetb0/5_epochs...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4dL1-QdOuiw7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}